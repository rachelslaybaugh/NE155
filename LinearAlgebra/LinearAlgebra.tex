%--------------------------------------------------------------------
% NE 155 (intro to numerical simulation of radiation transport)
% Spring 2014

% formatting
\documentclass[12pt]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\usepackage{setspace}
\onehalfspacing

\setlength{\parindent}{0mm} \setlength{\parskip}{1em}


% packages
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{times}
\renewcommand{\ttdefault}{cmtt}
\usepackage{amsmath}
\usepackage{graphicx} % for graphics files

% Draw figures yourself
\usepackage{tikz} 

% The float package HAS to load before hyperref
\usepackage{float} % for psuedocode formatting
\usepackage{xspace}

% from Denovo methods manual
\usepackage{mathrsfs}
\usepackage[mathcal]{euscript}
\usepackage{color}
\usepackage{array}

\usepackage[pdftex]{hyperref}

\newcommand{\nth}{n\ensuremath{^{\text{th}}} }
\newcommand{\ve}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\macro}{\ensuremath{\Sigma}}
\newcommand{\vOmega}{\ensuremath{\hat{\Omega}}}

\newcommand{\cc}[1]{\ensuremath{\overline{#1}}}
\newcommand{\ccm}[1]{\ensuremath{\overline{\mathbf{#1}}}}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\begin{document}
\begin{center}
{\bf NE 155, Classes 6 and 7, S14 \\
 Linear Algebra \\ February 3 and 5, 2014}
\end{center}

\setlength{\unitlength}{1in}
\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}


ADD MORE MEANING
%-------------------------------------------------------------
%-------------------------------------------------------------
\section{Vector Review}

A real $n$-dimensional vector $\vec{x}$ is an ordered set of $n$ real numbers that expresses magnitude and direction:
%
\begin{equation}
\vec{x} = (x_1, x_2, \dots, x_n) \nonumber
\end{equation}

\textbf{Properties}:
%
\begin{enumerate}
\item sum: two vectors of the same size give a new vector of that size: $\vec{x} + \vec{y} = (x_1 + y_1, x_2 + y_2, \dots, x_n + y_n)$

\item scalar multiple: $c\vec{x} = (cx_1, cx_2, \dots, cx_n)$

%\item Euclidean norm (length): $||\vec{x}|| = (x_1^2 + x_2^2 + \dots + x_n^2)^{1/2}$

\item dot product: takes two equal length vectors and results in a scalar. 

Algebraically, it is the sum of the products of the corresponding entries of the two sequences of numbers: $\vec{x} \cdot \vec{y} = \sum_{i=1}^n a_i b_i = x_1 y_1 + x_2 y_2 + \dots + x_n y_n$

Geometrically, it is the product of the magnitudes of the two vectors and the cosine of the angle between them. $\vec{x} \cdot \vec{y} = ||\vec{x}|| ||\vec{y}|| cos\theta$.

\item $||\vec{x}||^2 = \vec{x} \cdot \vec{x}$

\item distance from $\vec{x}$ to $\vec{y}$: $||\vec{x} - \vec{y}|| = ((x_1 - y_1)^2 + (x_2 - y_2)^2 + \dots + (x_n - y_n)^2)^{1/2}$

\item commutative property: $\vec{x} + \vec{y} = \vec{y} + \vec{x}$

\item associative property: $(\vec{x} + \vec{y}) + \vec{z} = \vec{x} + (\vec{y} + \vec{z})$

\item distributive property: $c(\vec{x} + \vec{y}) = c\vec{x} + c\vec{y}$
\end{enumerate}


%-------------------------------------------------------------
\subsection{Vector Norms}
Given $\vec{x}, \vec{y} \in \mathcal{R}^n$, a vector norm, denoted by $|| \cdot ||$, has the following properties:
%
\begin{enumerate}
\item $||\vec{x}|| > 0$; $||\vec{x}|| = 0$ iff $\vec{x} = 0$ (positive definite)
\item $||\vec{x} + \vec{y}|| \leq ||\vec{x}|| + ||\vec{y}||$ (triangle inequality)
\item $||\alpha \vec{x}|| = |\alpha| ||\vec{x}||$ (homogeneous)
\end{enumerate}

The p-norm:
%
\begin{equation}
||\vec{x}||_p \equiv (|x_1|^p + |x_2|^p + \dots + |x_n|^p)^{1/p} \qquad p \geq 1 \nonumber
\end{equation}
%
\begin{itemize}
\item $||\vec{x}||_1 \equiv |x_1| + |x_2| + \dots + |x_n|$
\item Euclidean norm (length) $||\vec{x}||_2 \equiv (|x_1|^2 + |x_2|^2 + \dots + |x_n|^2)^{1/2}$
\item $||\vec{x}||_{\infty} \equiv \displaystyle \max_{1 \leq i \leq n} |x_1|$
\end{itemize}

%-------------------------------------------------------------------
\subsection{Inner Products}
Inner products are symmetric and bilinear form in $\mathcal{R}^n$:
%
\begin{itemize}
\item bi-linear (depends on two arguments) $< \vec{x}, \vec{y} >$
\item symmetric $< \vec{x}, \vec{y} > = < \vec{y}, \vec{x} >$
\item linear $< \vec{x} + \alpha \vec{z}, \vec{y} > = < \vec{x}, \vec{y} > + \alpha < \vec{y}, \vec{z} >$
\item positive $< \vec{x}, \vec{x} >$ $>$ 0 for $\vec{x} \neq 0$
\end{itemize}

The dot product is also known as the Euclidean inner product. One can define many inner products, but in this class assume we're using the Euclidean inner product (dot product) unless otherwise specified. 

\subsection{Convergence}
We can use the concepts of norms and inner products to develop some relationships and talk about convergence. 

The Cauchy-Schwartz inequality states
%
\begin{equation}
< \vec{x}, \vec{y} > \leq ||\vec{x}||_2 ||\vec{y}||_2 \nonumber
\end{equation}
%
Two norms, denoted $|| \cdot ||_a$ and $|| \cdot ||_b$ here, are defined as being equivalent if there exists $C_1$ and $C_2$ such that $\forall \vec{x} \in \mathcal{R}^n$
%
\begin{equation}
C_1 ||\vec{x}||_a \leq ||\vec{x}||_b \leq C_2 ||\vec{x}||_a \qquad \text{where } C_1, C_2 = f(n) \nonumber
\end{equation}
%
This leads to the theorem that in $\mathcal{R}^n$ all norms are equivalent (offered without proof). E.g.:
\begin{align}
& ||\vec{x}||_{\infty} \leq ||\vec{x}||_2 \leq \sqrt[]{n} ||\vec{x}||_{\infty} \nonumber \\
%
& ||\vec{x}||_{\infty} \leq ||\vec{x}||_1 \leq n ||\vec{x}||_{\infty} \nonumber \\
%
& ||\vec{x}||_2 \leq ||\vec{x}||_1 \leq \sqrt[]{n} ||\vec{x}||_2 \nonumber
\end{align}

The inequalities above may not be `sharp'. A sharp inequality is when there could be no better inequality when making the comparison. E.g., when comparing two real numbers/expressions, an inequality is sharp because we could not increase the left or decrease the right by a positive factor and still have it be true ($2 \leq 2$). 

Example: let's show the last item. We will use the Signum function, which extracts the sign of a real number
\begin{align}
              & -1 \text{ if } x < 0 \nonumber \\
\text{sign}(x) \equiv &  0 \text{ if } x = 0 \nonumber \\
              &  1 \text{ if } x > 0 \nonumber
\end{align}
%
Let's define $v_i$ as sign$(x_i)$. Then
%
\begin{align}
&< \vec{x}, \vec{v} > = \sum_i x_i v_i = \sum_i x_i \text{sign}(x_i) = \sum_i |x_i| = ||\vec{x}||_1 \nonumber \\
%
&\text{but by the Cauchy-Schwartz inequality,}\nonumber \\
%
&< \vec{x}, \vec{v} > \leq ||\vec{x}||_2 ||\vec{v}||_2 \nonumber \\
%
&\text{and we can give an upper bound of } ||\vec{v}||_2 \leq \sqrt{n} \:, \text{ the right side of the inequality} \nonumber \\ 
%
&\text{we can also say } ||\vec{x}||_2 \leq ||\vec{v}||_2 ||\vec{x}||_2 \leq \sqrt{n} ||\vec{x}||_2 \nonumber \\
%
&\text{thus } ||\vec{x}||_2 \leq ||\vec{x}||_1 \leq \sqrt{n} ||\vec{x}||_2 \nonumber
\end{align}

NEED $||\vec{x}||_2 \leq ||\vec{x}||_1$ !!!!

Norm equivalence is very important because if we can prove some property (usually convergence or an error bound) in \textbf{some} norm, we have effectively proven it in \textbf{all} norms. 

\subsubsection{Error}
For $\vec{x}$ (the real solution) and $\hat{x}$ (representing our numerical solution) $\in \mathcal{R}^n$ and some norm p we define
%
\begin{itemize}
\item Absolute error: $||\hat{x} - \vec{x}||_p$
\item Relative error: $||\hat{x} - \vec{x}||_p / ||\vec{x}||$, where $\vec{x} \neq 0$
\end{itemize}

\subsubsection{Convergence}
Given a sequence $\lbrace x^{(k)} \rbrace_{k=1,2,\dots,\infty}$ and some norm, we say that $\lbrace x^{(k)} \rbrace$ converges to $\vec{x}$ if
%
\begin{equation}
\displaystyle\text{lim}_{k \rightarrow \infty} ||x^{(k)} - \vec{x}|| = 0 \nonumber
\end{equation}

%-------------------------------------------------------------
%-------------------------------------------------------------
\section{Matrices}

\begin{align}
    \ve{A} &= [a_{ij}]_{m\times n}   =    \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1j} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2j} & \cdots & a_{2n} \\
       \vdots & \vdots & \ddots & \vdots & \ddots   & \vdots \\     
      a_{31} & a_{32} & \cdots & a_{ij} & \cdots & a_{in} \\
      \vdots & \vdots & \ddots & \vdots & \ddots   & \vdots \\
      a_{m1} & a_{m2} & \cdots & a_{mj} & \cdots & a_{mn} \\
    \end{pmatrix} \nonumber   
\end{align} 
%
where $i = 1, \dots, m$ is the row index and $j = 1, \dots, n$ is the column index.

$\ve{A} \in \mathbb{R}^{m \times n}$ is an $m \times n$ real matrix\\
$\ve{A} \in \mathbb{C}^{m \times n}$ is an $m \times n$ complex matrix

\textbf{Properties}:
%
\begin{enumerate}
\item sum: $\ve{A} + \ve{B} = [a_{ij} + b_{ij}]_{m \times n}$

\item scalar multiple: $c\ve{A} = [c a_{ij}]_{m \times n}$

\item multiplication: $\ve{C} = \ve{A}\ve{B}$;

$\ve{A} \in \mathbb{C}^{m \times n}$, and $\ve{B} \in \mathbb{C}^{n \times p}$, and $\ve{C} \in \mathbb{C}^{m \times p}$, then $c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}$

$\ve{A}\ve{B} \neq \ve{B}\ve{A}$

\item commutative property: $\ve{A} + \ve{B} = \ve{B} + \ve{A}$

\item associative property: $(\ve{A} + \ve{B}) + \ve{C} = \ve{A} + (\ve{B} + \ve{C})$

\item distributive property: $c(\ve{A} + \ve{B}) = c\ve{A} + c\ve{B}$

\end{enumerate}


%-------------------------------------------------------------
\subsection{Definitions}

Given $\ve{A} \in \mathbb{C}^{m \times n}$, \ve{A} is
%
\begin{enumerate}
\item Transpose: $\ve{A} \in \mathbb{C}^{m \times n}$, and $\ve{B} \in \mathbb{C}^{n \times m} = \ve{A}^T from$

$b_{	ij} = a_{ji}$ for $i = 1, \dots, n$ and $j = 1, \dots, m$

\item Conjugate Transpose / adjoint, $\ve{A}^H = \cc{\ve{A}^T}$ is the complex conjugate of the transpose. 

\item Inverse: $\ve{AA}^{-1} = \ve{A}^{-1}\ve{A} = \ve{I}$, where $\ve{I}$ is a diagonal matrix containing ones on the diagonal.

If this exists, $\ve{A}$ is non-singular / invertible. If the inverse exists, it is unique.

\item Symmetric if $\ve{A} = \ve{A}^T$

\item Antisymmetric / skew-symmetric if $\ve{A}^H = -\ve{A}$

\item Hermitian / self-adjoint if $\ve{A} = \ve{A}^H$

\item Regular if $\ve{A}^{-1}$ exists

\item Unitary if $\ve{A}\ve{A}^H = \ve{I}$

\item Normal if $\ve{A}\ve{A}^H = \ve{A}^H\ve{A}$
\end{enumerate}


%-------------------------------------------------------------
\subsection{Equations and Special Matrices}

We often write systems of equations as $\ve{A}\vec{x} = \vec{b}$ from
\begin{align}
&a_{11} x_1 + a_{12} x_2 + \dots + a_{1n} x_n = b_1 \nonumber \\
&\vdots \nonumber \\
&a_{n1} x_1 + a_{n2} x_2 + \dots + a_{nn} x_n = b_n \nonumber
\end{align}

To find $\vec{x}$, we need to find a way to affect $\ve{A}^{-1}\vec{b}$. This can be done many many ways, and the first thing we'll talk about are methods for inverting the matrix.

\begin{itemize}
\item Tridiagonal matrix has entries on only the main, upper, and lower diagonal
\item Lower triangular has entries on the diagonal and below
\item Upper triangular has entries on the diagonal and above 
\item Block Tridagonal has blocks of elements (like sub-matrices) on the diagonal. The blocks may be full or only partially full. The blocks look like $\ve{D}_k = [\ve{D_{ij}}]_k$
\end{itemize}

The inverse of a diagonal is simply: $d_{ii}^{-1} = 1/d_{ii}$, another diagonal matrix

\textbf{Theorem}: The following are equivalent (see Math 54 or a textbook for proof)
%
\begin{enumerate}
\item $\ve{A}$ is regular
\item Rank($\ve{A}$) = n
\item $\ve{A}\vec{x} = 0$ iff $x=0$
\item $\ve{A}\vec{x} = \vec{b}$ is uniquely solveable $\forall \vec{b}$
\item det($\ve{A}) \neq 0$
\end{enumerate}


%-------------------------------------------------------------
\subsection{Minors, Cofactors, Determinants}

Good illustration: http://www.mathsisfun.com/algebra/matrix-inverse-minors-cofactors-adjugate.html

%If $\ve{A}$ is an $m \times n$ matrix and $k$ is an integer such that $0 < k \leq m$ and $k \leq n$ then a \textbf{$k \times k$ minor}, $M_{kk}$ of $\ve{A}$ is the determinant of the $k \times k$ matrix formed by deleting $m-k$ rows and $n-k$ columns. 

For a square matrix, the \textbf{first order minor}, $M_{ij}$ just deletes the $i^{th}$ row and $j^{th}$ column and takes the determinant. E.g.
%
\begin{align}
    \ve{A} &= \begin{pmatrix}
        1 & 4 & 7 \\
        3 & 0 & 5 \\
        -1 & 9 & 11 \\
    \end{pmatrix} 
    \qquad
    &M_{23} = \text{det}\begin{pmatrix}
       1 & 4 \\
       -1 & 9 
    \end{pmatrix}   
    = ((1 \times 9) - (4 \times -1)) = 13 \nonumber
\end{align} 

The corresponding $i,j$ \textbf{cofactor} of $\ve{A}$ is
%
\begin{equation}
C_{ij} = (-1)^{i+j} M_{ij} \nonumber
\end{equation}
%
You can compute minors and cofactors for all of $\ve{A}$. The $n \times n$ matrix containing all of the cofactors is denoted $\ve{C}$ in this context.

Using these terms, the determinant can be defined in terms of the Laplace expansion
%
\begin{equation}
\text{det}(\ve{A}) = \sum_{j=1}^n a_{ij} C_{ij} = \sum_{i=1}^n a_{ij} C_{ij} \nonumber
\end{equation}

For the above matrix, lets look at the Laplace expansion along the second column ($j = 2$; sum runs over $i$)
\begin{align}
\text{det}(\ve{A}) &= (-1)^{1+2} a_{12} M_{12} + (-1)^{2+2} a_{22} M_{22} + (-1)^{3+2} a_{32} M_{32} \nonumber \\
%
&= (-1)^{1+2} \cdot 4 \cdot \text{det}\begin{pmatrix}
        3 & 5 \\
        -1 & 11 \\ \end{pmatrix} + (-1)^{2+2} \cdot 0 \cdot \text{det}\begin{pmatrix} 
        1 & 7 \\
        -1 & 11 \\ \end{pmatrix} + (-1)^{3+2} \cdot 9 \cdot \text{det}\begin{pmatrix} 
        1 & 7 \\
        3 & 5 \\\end{pmatrix} \nonumber \\
%
&= -4 \cdot ((3 \cdot 11) - (5 \cdot -1)) + 0 -9 \cdot ((1 \cdot 5) - (7 \cdot 3)) = -8 \nonumber
\end{align}


\textbf{Properties of Determinants}
\begin{itemize}
\item det($\alpha\ve{A}$) = $\alpha^N$ det($\ve{A}$)

\item det($\ve{A}^T$) = det($\ve{A}$)

\item det($\ve{A}^{-1}$) = 1/det($\ve{A}$)

\item det($\ve{AB}$) = det($\ve{A}$)det($\ve{B}$)

\item det($\ve{A}^n$) = [det($\ve{A}$)]$^n$

\item in general, det($\ve{A + B}$) $\neq$ det($\ve{A}$) + det($\ve{B}$) 
\end{itemize}

The inverse of $\ve{A}$ can be obtained from the determinant and cofactor matrix:
%
\begin{equation}
\ve{A}^{-1} = \frac{1}{\text{det}(\ve{A})}\ve{C}^T \nonumber
\end{equation}
%
If the determinant is zero, then the matrix is singular.

A real matrix where $\ve{A}^{-1} = \ve{A}^T$ is called \textbf{orthogonal}


%--------------------------------------------------------------------------
\subsection{Matrix Norms}
Given $\ve{A}, \ve{B} \in \mathcal{R}^{m \times n}$, a matrix norm, denoted by $|| \cdot ||$, has the following properties:
%
\begin{enumerate}
\item $||\ve{A}|| > 0$ and $||\ve{A}|| = 0$ iff $\ve{A} = 0$ (positive definite)
\item $||\ve{A} + \ve{B}|| \leq ||\ve{A}|| + ||\ve{B}||$ (triangle inequality)
\item $||\alpha \ve{A}|| = |\alpha| ||\ve{A}||$ (homogeneous)
\end{enumerate}

E.g., the Frobenius norm 
%
\begin{equation}
||\ve{A}||_F = \sqrt{ \sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2 } \nonumber
\end{equation}

\textbf{Definitions}:
\begin{itemize}
\item submultiplicative if $||\ve{A} \ve{B}|| \leq ||\ve{A}|| ||\ve{B}||$

\item Not all norms are submultiplicative, we will only deal with those that are. 

\item subordinate matrix norm for $\ve{A} \in \mathcal{R}_{m \times n}$ and $\vec{x} \in \mathcal{R}_n$:

$||\ve{A}|| \equiv \displaystyle \sup_{\vec{x} \neq \vec{0}} ||\ve{A}\vec{x}|| / ||\vec{x}||$

\item \textbf{supremum}, or least upper bound, of a set S of real numbers is denoted by sup S and is defined to be the smallest real number that is greater than or equal to every number in S.
\end{itemize}

ADD MORE INFO ABOUT SUP CHOICE/MEANING

\textbf{examples:}
%
\begin{align}
||\ve{A}||_{1} &= \displaystyle \sup_{\vec{x} \neq \vec{0}} \frac{||\ve{A}\vec{x}||_{1}}{||\vec{x}||_{1}} =
\displaystyle \max_{1 \leq j \leq n} \sum_{i=1}^m |a_{ij}| \qquad \text{max absolute col sum} \nonumber \\
%
||\ve{A}||_{\infty} &= \displaystyle \sup_{\vec{x} \neq \vec{0}} \frac{||\ve{A}\vec{x}||_{\infty}}{||\vec{x}||_{\infty}} = 
\displaystyle \max_{1 \leq i \leq m} \sum_{j=1}^n |a_{ij}| \qquad \text{max absolute row sum}\nonumber \\
%
||\ve{A}||_{2} &= \displaystyle \sup_{\vec{x} \neq \vec{0}} \frac{||\ve{A}\vec{x}||_{2}}{||\vec{x}||_{2}} = \text{ the spectral radius of }\ve{A} \nonumber 
\end{align}

The equivalence of norms we talked about with vectors also holds here. Some of the relationships are
%
\begin{align}
||\ve{A}||_{2} &\leq ||\ve{A}||_{F} \leq \sqrt{n}||\ve{A}||_{2} \nonumber \\
%
\frac{1}{\sqrt{n}}||\ve{A}||_{\infty} &\leq ||\ve{A}||_{2} \leq \sqrt{m}||\ve{A}||_{\infty} \nonumber \\
%
\frac{1}{\sqrt{m}}||\ve{A}||_{1} &\leq ||\ve{A}||_{2} \leq \sqrt{n}||\ve{A}||_{2} \nonumber
\end{align}

Again, as with vector norms, showing a property in some matrix norms implies that property in other matrix norms (which may be harder to compute). The inequalities above may not be `sharp'.


%--------------------------------------------------------------------------
%--------------------------------------------------------------------------
\section{Eigenvalues}

Eigenvalues are a special set of scalars associated with a linear system of equations (a matrix equation) that are sometimes also known as characteristic roots. 

Each eigenvalue is paired with a corresponding so-called eigenvector (or, in general, a corresponding right eigenvector and a corresponding left eigenvector; there is no analogous distinction between left and right for eigenvalues).

The decomposition of a square matrix $\ve{A}$ into eigenvalues and eigenvectors is known as eigen decomposition, and the fact that \textbf{this decomposition is always possible as long as the matrix consisting of the eigenvectors of $\ve{A}$ is square} is known as the eigen decomposition theorem.

Let $\ve{A} \in \mathcal{R}^{n \times n}$. If there is a vector $\vec{x} \in \mathcal{R}^{n}$ such that
%
\[\ve{A} \vec{x} = \lambda \vec{x}\]
%
for some scalar $\lambda$, then $\lambda$ is an eigenvalue of $\ve{A}$ with a corresponding (right) eigenvector $\vec{x}$.
 
We find eigenvalues by re-writing the stated relationship as $(\ve{A} - \lambda \ve{I})\vec{x}=0$.

A linear system of equations has nontrivial solutions iff the determinant vanishes (Cramer's rule), so the solutions of this equation are given by
%
\[\det(\ve{A} - \lambda \ve{I})=0 \:.\] 	
%
This equation is known as the characteristic equation of $\ve{A}$, and the left-hand side is known as the characteristic polynomial.

\textbf{Example}
\begin{align}
    \ve{A} &= \begin{pmatrix}
        2 & 3 \\
        3 & -6
    \end{pmatrix} 
    &\ve{A} - \lambda \ve{I} = \text{det}\begin{pmatrix}
        2 - \lambda & 3 \\
        3 & -6 - \lambda
    \end{pmatrix} \nonumber \\
%  
\det(\ve{A} - \lambda \vec{I}) &= (2 - \lambda)(-6 - \lambda) - (3)(3) \nonumber \\
%
&= -12 + 6\lambda -2\lambda + \lambda^2 -9 \nonumber \\
&= \lambda^2 + 4\lambda -21 = 0 \nonumber \\
0 &= (\lambda - 3)(\lambda + 7) \nonumber \\
&\boxed{\lambda = 3, -7} \nonumber
\end{align} 


\begin{itemize}
\item Eigenvalues may be real or complex. 

\item Since an \nth degree polynomial has $n$ roots, $\ve{A}_{n \times n}$ is guaranteed to have $n$ real and/or complex eigenvalues, some of which may be repeated: $\lambda_1, \lambda_2, \dots, \lambda_n$.

This gives $\ve{A}\vec{u}_1 = \lambda_1 \vec{u}_1$, etc.

\item Left eigenvectors are found by reformulating the equation as $\vec{y}\ve{A} = \alpha \vec{y}$. In nuclear, we're used to seeing the right eigenvector formulation.
\end{itemize}


The spectrum of eigenvalues of a matrix $\ve{A}$ are defined formally as
\[\sigma(\ve{A}) = [ \lambda \in \mathcal{C} : \det(\ve{A} - \lambda \ve{I})=0] \] 
and an eigenvalue is 
\[ \lambda \in \sigma(\ve{A}) \]
%
\begin{itemize}
\item $\sigma(\ve{A}) = \sigma(\ve{A}^T)$
\item $\cc{\sigma(\ve{A})} = \sigma(\ve{A}^H)$
\end{itemize}

%--------------------------------------------------------------------------
\subsection{Aside about Singular Values}

\underline{Note:} Eigenvalues/vectors are only for square matrices. The analog for rectangular matrices is singular values. We are not going to focus on these, since we will be dealing with square matrices. However, this came up last class and appeared to cause some confusion. So, here's a quick aside:

If a square matrix $\ve{A}$ has linearly independent eigenvectors, it can be factored as $\ve{A} = \ve{P}\ve{D}\ve{P}^{-1}$. Here $\ve{D}$ is a diagonal matrix containing the eigenvalues of $\ve{A}$ and $\ve{P}$ contains the corresponding, linearly independent eigenvectors of $\ve{A}$.  

You cannot do this for a rectangular matrix. However, \textbf{any} matrix $\ve{A} \in \mathcal{R}^{m \times n}$ can be factored as $\ve{Q}\ve{D}\ve{P}^{-1}$, and this is called the singular value decomposition. 

This comes from the notion that $\ve{A}^T\ve{A}$ is symmetric and can be orthogonally diagonalized (get linearly-independent eigenvectors). 

The singular values of $\ve{A}$ are the square roots of the eigenvalues of $\ve{A}^T\ve{A}$, and are denoted by $\sigma_1, \sigma_2, \dots, \sigma_n$. 

%--------------------------------------------------------------------------
\subsection{Spectral radius} 

The spectral radius of $\ve{A}$ is defined as 
\[\rho(\ve{A}) \equiv \max \lbrace |\lambda|, \lambda \in \sigma(\ve{A})\]
With the properties:
%
\begin{itemize}
\item $\rho(\ve{A})^k = \rho(\ve{A}^k)$
\item $\rho(\ve{A}) = \rho(\ve{A}^T)$
\item $\rho(\xi \ve{A})^k = |\xi| \rho(\ve{A})$
\end{itemize}

%\textbf{example}
%Show that the eigenvalues of a Hermitian matrix are real, noting that the eigenvalues of $\ve{A}$ satisfy
%\[\ve{A}\vec{x} = \lambda x \qquad \text{and } \vec{x} \neq \vec{0}\]
%and recall that $\ve{A} = \ve{A}^H = \ccm{A}^T$ defines a Hermitian matrix.
%%
%\begin{align}
%\cc{u}^T \ccm{A}^T &= \cc{\lambda} \cc{u}^T
%\qquad \text{The first step} \nonumber \\
%%
%\cc{u}^T \ve{A} &= \cc{\lambda} \cc{u}^T 
%\qquad \text{apply Hermitian definition}\nonumber \\
%%
%\cc{u}^T \ve{A} \vec{u} &= \cc{\lambda} \cc{u}^T \vec{u}
%\qquad \text{right multiply by }\vec{u}\nonumber \\
%%
%\cc{u}^T \lambda \vec{u} &= \cc{\lambda} \cc{u}^T \vec{u}
%\qquad \text{apply eigenvalue definition }\nonumber \\
%%
%\lambda &= \cc{\lambda} 
%\qquad \text{recognize that the }\vec{u}\text{s factor out}\nonumber
%\end{align}
 
 %--------------------------------------------------------------------------
\subsection{All matrix norms are bounded}

For $\lambda \in \sigma(\ve{A})$ 
\[|\lambda | \leq ||\ve{A}||\]
\[\rho(\ve{A}) \leq ||\ve{A}||\]

\underline{proof:} 
\begin{enumerate}
\item The eigenvalues of $\ve{A}$ satisfy $\ve{A}\vec{x} = \lambda x$, $\vec{x} \neq \vec{0}$. 

\item Let the eigenvectors of $\ve{A}$ be $\vec{v} = [\vec{v_1}, \vec{v_2}, \dots, \vec{v_n}]$

\item then 
\begin{align}
|\lambda | ||\vec{v}|| &= ||\lambda \vec{v}|| 
\qquad \text{property of norms} \nonumber \\
%
                     &= ||\ve{A}\vec{v}|| 
                     \qquad \text{substitution} \nonumber \\
%
                     &\leq ||\ve{A}|| ||\vec{v}||
                     \qquad \text{triangle inequality} \nonumber \\
%
\therefore |\lambda | &\leq ||\ve{A}|| 
\qquad \text{cancellation}\nonumber \\
%
|\lambda| &\leq \rho(\ve{A}) \leq ||\ve{A}|| 
\qquad \text{definition of }\rho\text{ as max }|\lambda | \nonumber
\end{align}
\end{enumerate}

Note, we can use any submultiplicative matrix norm to bound from above the spectral radius (This will be important).


\end{document}