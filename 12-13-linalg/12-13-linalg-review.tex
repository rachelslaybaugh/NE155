%--------------------------------------------------------------------
% NE 155 (intro to numerical simulation of radiation transport)

% formatting
\documentclass[12pt]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\usepackage{setspace}
\onehalfspacing

\setlength{\parindent}{0mm} \setlength{\parskip}{1em}

% packages
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{times}
\renewcommand{\ttdefault}{cmtt}
\usepackage{amsmath}
\usepackage{graphicx} % for graphics files

% Draw figures yourself
\usepackage{tikz} 

% The float package HAS to load before hyperref
\usepackage{float} % for psuedocode formatting
\usepackage{xspace}

% from Denovo methods manual
\usepackage{mathrsfs}
\usepackage[mathcal]{euscript}
\usepackage{color}
\usepackage{array}

\usepackage[pdftex]{hyperref}

\newcommand{\nth}{n\ensuremath{^{\text{th}}} }
\newcommand{\ve}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\macro}{\ensuremath{\Sigma}}
\newcommand{\vOmega}{\ensuremath{\hat{\Omega}}}

\newcommand{\cc}[1]{\ensuremath{\overline{#1}}}
\newcommand{\ccm}[1]{\ensuremath{\overline{\mathbf{#1}}}}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\begin{document}
\begin{center}
{\bf NE 155, Class 12 and 13, S16 \\
 Vectors and Matrices: Feb 17-19, 2016}
\end{center}

\setlength{\unitlength}{1in}
\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}
 
%-------------------------------------------------------------
%-------------------------------------------------------------
\section*{Vector Review}

A real $n$-dimensional vector $\vec{x}$ is an ordered set of $n$ real numbers that expresses magnitude and direction:
%
\begin{equation}
\vec{x} = (x_1, x_2, \dots, x_n) \nonumber
\end{equation}

\textbf{Properties}:
%
\begin{enumerate}
\item sum: two vectors of the same size give a new vector of that size: $\vec{x} + \vec{y} = (x_1 + y_1, x_2 + y_2, \dots, x_n + y_n)$

\item scalar multiple: $c\vec{x} = (cx_1, cx_2, \dots, cx_n)$

%\item Euclidean norm (length): $||\vec{x}|| = (x_1^2 + x_2^2 + \dots + x_n^2)^{1/2}$

\item dot product: takes two equal length vectors and results in a scalar. 

Algebraically, it is the sum of the products of the corresponding entries of the two sequences of numbers: $\vec{x} \cdot \vec{y} = \sum_{i=1}^n a_i b_i = x_1 y_1 + x_2 y_2 + \dots + x_n y_n$

Geometrically, it is the product of the magnitudes of the two vectors and the cosine of the angle between them. $\vec{x} \cdot \vec{y} = ||\vec{x}|| ||\vec{y}|| cos\theta$.

\item $||\vec{x}||^2 = \vec{x} \cdot \vec{x}$

\item distance from $\vec{x}$ to $\vec{y}$: $||\vec{x} - \vec{y}|| = ((x_1 - y_1)^2 + (x_2 - y_2)^2 + \dots + (x_n - y_n)^2)^{1/2}$

\item commutative property: $\vec{x} + \vec{y} = \vec{y} + \vec{x}$

\item associative property: $(\vec{x} + \vec{y}) + \vec{z} = \vec{x} + (\vec{y} + \vec{z})$

\item distributive property: $c(\vec{x} + \vec{y}) = c\vec{x} + c\vec{y}$
\end{enumerate}

%-------------------------------------------------------------
%-------------------------------------------------------------
\section*{Matrix Review}

\begin{align}
    \ve{A} &= [a_{ij}]_{m\times n}   =    \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1j} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2j} & \cdots & a_{2n} \\
       \vdots & \vdots & \ddots & \vdots & \ddots   & \vdots \\     
      a_{i1} & a_{i2} & \cdots & a_{ij} & \cdots & a_{in} \\
      \vdots & \vdots & \ddots & \vdots & \ddots   & \vdots \\
      a_{m1} & a_{m2} & \cdots & a_{mj} & \cdots & a_{mn} \\
    \end{pmatrix} \nonumber   
\end{align} 
%
where $i = 1, \dots, m$ is the row index and $j = 1, \dots, n$ is the column index.

$\ve{A} \in \mathbb{R}^{m \times n}$ is an $m \times n$ real matrix\\
$\ve{A} \in \mathbb{C}^{m \times n}$ is an $m \times n$ complex matrix

\textbf{Properties}:
%
\begin{enumerate}
\item sum: $\ve{A} + \ve{B} = [a_{ij} + b_{ij}]_{m \times n}$

\item scalar multiple: $c\ve{A} = [c a_{ij}]_{m \times n}$

\item multiplication: $\ve{C} = \ve{A}\ve{B}$;

$\ve{A} \in \mathbb{C}^{m \times n}$, and $\ve{B} \in \mathbb{C}^{n \times p}$, and $\ve{C} \in \mathbb{C}^{m \times p}$, then $c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}$

$\ve{A}\ve{B} \neq \ve{B}\ve{A}$

\item commutative property: $\ve{A} + \ve{B} = \ve{B} + \ve{A}$

\item associative property: $(\ve{A} + \ve{B}) + \ve{C} = \ve{A} + (\ve{B} + \ve{C})$

\item distributive property: $c(\ve{A} + \ve{B}) = c\ve{A} + c\ve{B}$

\end{enumerate}


%-------------------------------------------------------------
\subsection*{Definitions}

Given $\ve{A} \in \mathbb{C}^{m \times n}$, \ve{A} is
%
\begin{enumerate}
\item Transpose: $\ve{A} \in \mathbb{C}^{m \times n}$, and $\ve{B} \in \mathbb{C}^{n \times m} = \ve{A}^T$ from

$b_{ij} = a_{ji}$ for $i = 1, \dots, n$ and $j = 1, \dots, m$
%
\newcommand{\aaa}{1-i}
\newcommand{\aab}{2}
\newcommand{\aba}{3+2i}
\newcommand{\abb}{4}
\newcommand{\aca}{5}
\newcommand{\acb}{6+0.4i}
%
\begin{equation}
\ve{A} = \begin{pmatrix}
   \aaa & \aab \\
   \aba & \abb \\
   \aca & \acb \\
\end{pmatrix}\:, \qquad 
%
\ve{A}^T = \begin{pmatrix}
   \aaa & \aba & \aca \\
   \aab & \abb & \acb \\
\end{pmatrix} \nonumber
\end{equation}

%--------------------
\item Conjugate Transpose / adjoint, $\ve{A}^H = \cc{\ve{A}^T}$ is the complex conjugate of the transpose. 

Recall that complex conjugates are a pair of complex numbers, both the same except with imaginary parts of opposite signs. For example, The conjugate of the complex number $z=a+ib$, where $a$ and $b$ are real numbers, is $\overline{z} = a - ib$.
%http://en.wikipedia.org/wiki/Complex_conjugate
\renewcommand{\aaa}{1+i}
\renewcommand{\aba}{3-2i}
\renewcommand{\acb}{6-i}
%
\begin{equation}
\ve{A}^H = \begin{pmatrix}
   \aaa & \aba & \aca \\
   \aab & \abb & \acb \\
\end{pmatrix} \nonumber
\end{equation}

%--------------------
\item Inverse: $\ve{AA}^{-1} = \ve{A}^{-1}\ve{A} = \ve{I}$, where $\ve{I}$ is a diagonal matrix containing ones on the diagonal. If this exists, $\ve{A}$ is non-singular / invertible. 
\renewcommand{\aaa}{4}
\renewcommand{\aab}{3}
\renewcommand{\aba}{3}
\renewcommand{\abb}{2}
%
\begin{equation}
\ve{A} = \begin{pmatrix}
   \aaa & \aab \\
   \aba & \abb 
\end{pmatrix} \nonumber
\end{equation}
%
\renewcommand{\aaa}{-2}
\renewcommand{\abb}{-4}
%
\begin{equation}
\ve{A}^{-1} = \begin{pmatrix}
   \aaa & \aab \\
   \aba & \abb 
\end{pmatrix} \nonumber
\end{equation}


%--------------------
\item Regular if $\ve{A}^{-1}$ exists

%--------------------
\item Hermitian / self-adjoint if $\ve{A} = \ve{A}^H$
%
\renewcommand{\aaa}{2}
\renewcommand{\aab}{2+i}
\newcommand{\aac}{4}
\renewcommand{\aba}{2-i}
\renewcommand{\abb}{3}
\newcommand{\abc}{i}
\renewcommand{\aca}{4}
\renewcommand{\acb}{-i}
\newcommand{\acc}{1}
%
\begin{equation}
\ve{A} = \ve{A}^H= \begin{pmatrix}
   \aaa & \aab & \aac \\
   \aba & \abb & \abc \\
   \aca & \acb & \acc \\
\end{pmatrix} \nonumber
\end{equation}

%--------------------
\item Symmetric if $\ve{A} = \ve{A}^T$
%
\renewcommand{\aaa}{1}
\renewcommand{\aab}{2}
\renewcommand{\aba}{2}
\renewcommand{\abb}{3}
%
\begin{equation}
\ve{A} = \ve{A}^T = \begin{pmatrix}
   \aaa & \aab \\
   \aba & \abb 
\end{pmatrix} \nonumber
\end{equation}

%--------------------
\item Antisymmetric / skew-symmetric if $\ve{A}^T = -\ve{A}$ (or, skew-Hermitian if $\ve{A}^H = -\ve{A}$)
%
\renewcommand{\aaa}{0}
\renewcommand{\aab}{-2}
\renewcommand{\aba}{2}
\renewcommand{\abb}{0}
%
\begin{equation}
\ve{A} = \begin{pmatrix}
   \aaa & \aab \\
   \aba & \abb 
\end{pmatrix}\:, \qquad
%
\ve{A}^T = \begin{pmatrix}
   \aaa & \aba \\
   \aab & \abb 
\end{pmatrix} \nonumber
\end{equation}

%--------------------
\item Unitary if $\ve{A}\ve{A}^H = \ve{I}$
%
%http://college.cengage.com/mathematics/larson/elementary_linear/4e/shared/downloads/c08s5.pdf
\begin{equation}
\ve{A} = \frac{1}{2}\begin{pmatrix}
   1+i & 1-i \\
   1-i & 1+i 
\end{pmatrix}\:,\nonumber
\end{equation}
%
\begin{equation}
\ve{AA}^H = \frac{1}{2}\begin{pmatrix}
   1-i & 1+i \\
   1+i & 1-i 
\end{pmatrix}  \times
\frac{1}{2}\begin{pmatrix}
   1+i & 1-i \\
   1-i & 1+i 
\end{pmatrix} =
\frac{1}{4}\begin{pmatrix}
   4 & 0 \\
   0 & 4 
\end{pmatrix} = \ve{I}\nonumber
\end{equation}

%--------------------
\item Normal if $\ve{A}\ve{A}^H = \ve{A}^H\ve{A}$
%
\begin{equation}
\ve{A} = \begin{pmatrix}
   1 & 1 & 0 \\
   0 & 1 & 1 \\
   1 & 0 & 1
\end{pmatrix}\:, \qquad
\ve{A}^H = \begin{pmatrix}
   1 & 0 & 1 \\
   1 & 1 & 0 \\
   0 & 1 & 1
\end{pmatrix}\:,\nonumber
\end{equation}
%
\begin{equation}
\ve{AA}^H = \begin{pmatrix}
   1 & 1 & 0 \\
   1 & 1 & 0 \\
   0 & 1 & 1 
\end{pmatrix}  \times
\begin{pmatrix}
   1 & 0 & 1 \\
   1 & 1 & 0 \\
   0 & 1 & 1 
\end{pmatrix} =
\begin{pmatrix}
   2 & 1 & 1 \\
   1 & 2 & 1 \\
   1 & 1 & 2 
\end{pmatrix} = \ve{A}^H\ve{A}\nonumber
\end{equation}


%--------------------
\item Orthogonal if A is real and $\ve{AA}^T = \ve{A}^T\ve{A}$, which means $\ve{A}^{-1} = \ve{A}^T$ .
%
\begin{equation}
\ve{A} = \frac{1}{\sqrt{2}}\begin{pmatrix}
   1 & 1 \\
   1 & -1 
\end{pmatrix}\:, \qquad
\ve{A}^T = \frac{1}{\sqrt{2}}\begin{pmatrix}
   1 & 1 \\
   1 & -1 
\end{pmatrix}\:,\qquad
\ve{A}^T\ve{A} = \frac{1}{2}\begin{pmatrix}
   2 & 0 \\
   0 & 2 
\end{pmatrix} = \ve{I} \nonumber
\end{equation}

\end{enumerate}

Among complex matrices, all unitary ($\ve{AA}^H = \ve{I}$), Hermitian/self-adjoint ($\ve{A} = \ve{A}^H$), and skew-Hermitian ($\ve{A}^H = -\ve{A}$) matrices are normal (($\ve{AA}^H = \ve{A}^H\ve{A}$)). 

Likewise, among real matrices, all orthogonal ($\ve{AA}^T = \ve{A}^T\ve{A}$), symmetric ($\ve{A} = \ve{A}^T$), and skew-symmetric ($\ve{A}^T = -\ve{A}$) matrices are normal. 

However, it is \textit{not} the case that all normal matrices are either unitary or (skew-)Hermitian (see example above).% http://en.wikipedia.org/wiki/Normal_matrix

%-------------------------------------------------------------
\subsection*{Equations and Special Matrices}

All of the information above is context to help us solve actual problems. 

We often write systems of equations as $\ve{A}\vec{x} = \vec{b}$ from
\begin{align}
&a_{11} x_1 + a_{12} x_2 + \dots + a_{1n} x_n = b_1 \nonumber \\
&\vdots \nonumber \\
&a_{n1} x_1 + a_{n2} x_2 + \dots + a_{nn} x_n = b_n \nonumber
\end{align}

To find $\vec{x}$, we need to find a way to affect $\ve{A}^{-1}\vec{b}$. This can be done many many ways, and the first thing we'll talk about are methods for inverting the matrix.

\begin{itemize}
\item Tridiagonal matrix has entries on only the main, upper, and lower diagonal
\item Lower triangular has entries on the diagonal and below
\item Upper triangular has entries on the diagonal and above 
\item Block Tridagonal has blocks of elements (like sub-matrices) on the diagonal. The blocks may be full or only partially full. The blocks look like $\ve{D}_k = [\ve{D_{ij}}]_k$
\end{itemize}

The inverse of a diagonal is simply: $d_{ii}^{-1} = 1/d_{ii}$, another diagonal matrix.

\textbf{Theorem}: The following are equivalent (see Math 54 or a textbook for proof):
%
\begin{enumerate}
\item $\ve{A}$ is regular ($\ve{A}^{-1}$ exists)
\item Rank($\ve{A}$) = n
\item $\ve{A}\vec{x} = 0$ iff $x=0$
\item $\ve{A}\vec{x} = \vec{b}$ is uniquely solveable $\forall \vec{b}$
\item det($\ve{A}) \neq 0$
\end{enumerate}


%-------------------------------------------------------------
\subsection*{Minors, Cofactors, Determinants}
If the determinant is zero, then the matrix is singular, meaning we cannot invert it and numerical solutions are pretty much impossible.

\textbf{Properties of Determinants}
\begin{itemize}
\item det($\alpha\ve{A}$) = $\alpha^n$ det($\ve{A}$)

\item det($\ve{A}^T$) = det($\ve{A}$)

\item det($\ve{A}^{-1}$) = 1/det($\ve{A}$)

\item det($\ve{AB}$) = det($\ve{A}$)det($\ve{B}$)

\item det($\ve{A}^k$) = [det($\ve{A}$)]$^k$

\item in general, det($\ve{A + B}$) $\neq$ det($\ve{A}$) + det($\ve{B}$) 
\end{itemize}

Good illustration: \href{http://www.mathsisfun.com/algebra/matrix-inverse-minors-cofactors-adjugate.html}{http://www.mathsisfun.com/algebra/matrix-inverse-minors-cofactors-adjugate.html}

%If $\ve{A}$ is an $m \times n$ matrix and $k$ is an integer such that $0 < k \leq m$ and $k \leq n$ then a \textbf{$k \times k$ minor}, $M_{kk}$ of $\ve{A}$ is the determinant of the $k \times k$ matrix formed by deleting $m-k$ rows and $n-k$ columns. 

For a square matrix, the \textbf{first order minor}, $M_{ij}$, just deletes the $i^{th}$ row and $j^{th}$ column and takes the determinant. E.g.
%
\begin{align}
    \ve{A} &= \begin{pmatrix}
        1 & 4 & 7 \\
        3 & 0 & 5 \\
        -1 & 9 & 11 \\
    \end{pmatrix} 
    \qquad
    &M_{23} = \text{det}\begin{pmatrix}
       1 & 4 \\
       -1 & 9 
    \end{pmatrix}   
    = ((1 \times 9) - (4 \times -1)) = 13 \nonumber
\end{align} 

The corresponding $i,j$ \textbf{cofactor} of $\ve{A}$ is
%
\begin{equation}
C_{ij} = (-1)^{i+j} M_{ij} \nonumber
\end{equation}
%
You can compute minors and cofactors for all of $\ve{A}$. The $n \times n$ matrix containing all of the cofactors is denoted $\ve{C}$ in this context.

Using these terms, the \underline{determinant} (which we use for lots of stuff) can be defined in terms of the Laplace expansion
%
\begin{align*}
\text{det}(\ve{A}) &= \sum_{j=1}^n a_{ij} C_{ij} \text{ for any i} \in \{1,...,n\} \\
%
&= \sum_{i=1}^n a_{ij} C_{ij} \text{ for any j} \in \{1,...,n\}
\end{align*}

For the above matrix, lets look at the Laplace expansion along the second column ($j = 2$; sum runs over $i$)
\begin{align}
\text{det}(\ve{A}) &= (-1)^{1+2} a_{12} M_{12} + (-1)^{2+2} a_{22} M_{22} + (-1)^{3+2} a_{32} M_{32} \nonumber \\
%
&= (-1)^{1+2} \cdot 4 \cdot \text{det}\begin{pmatrix}
        3 & 5 \\
        -1 & 11 \\ \end{pmatrix} + (-1)^{2+2} \cdot 0 \cdot \text{det}\begin{pmatrix} 
        1 & 7 \\
        -1 & 11 \\ \end{pmatrix} + (-1)^{3+2} \cdot 9 \cdot \text{det}\begin{pmatrix} 
        1 & 7 \\
        3 & 5 \\\end{pmatrix} \nonumber \\
%
&= -4 \cdot ((3 \cdot 11) - (5 \cdot -1)) + 0 -9 \cdot ((1 \cdot 5) - (7 \cdot 3)) = -8 \nonumber
\end{align}

The inverse of $\ve{A}$ (which we often need) can be obtained from the determinant and cofactor matrix:
%
\begin{equation}
\ve{A}^{-1} = \frac{1}{\text{det}(\ve{A})}\ve{C}^T \nonumber
\end{equation}
%
The benefit is that if we've used the cofactor method to get the determinant then we get the inverse for free. 


\section*{Norms and Convergence}
We're going to look at direct and iterative methods to solve problems; we'll need these concepts to understand how the solution methods behave.

%-------------------------------------------------------------
\subsection*{Vector Norms}
Given $\vec{x}, \vec{y} \in \mathbb{R}^n$, a vector norm, denoted by $|| \cdot ||$, has the following properties:
%
\begin{enumerate}
\item $||\vec{x}|| > 0$; $||\vec{x}|| = 0$ iff $\vec{x} = 0$ (positive definite)
\item $||\vec{x} + \vec{y}|| \leq ||\vec{x}|| + ||\vec{y}||$ (triangle inequality)
\item $||\alpha \vec{x}|| = |\alpha| ||\vec{x}||$ (homogeneous)
\end{enumerate}

The p-norm:
%
\begin{equation}
||\vec{x}||_p \equiv (|x_1|^p + |x_2|^p + \dots + |x_n|^p)^{1/p} \qquad p \geq 1 \nonumber
\end{equation}
%
\begin{itemize}
\item $||\vec{x}||_1 \equiv |x_1| + |x_2| + \dots + |x_n|$
\item Euclidean norm (length) $||\vec{x}||_2 \equiv (|x_1|^2 + |x_2|^2 + \dots + |x_n|^2)^{1/2}$
\item $||\vec{x}||_{\infty} \equiv \displaystyle \max_{1 \leq i \leq n} |x_i|$
\end{itemize}

%-------------------------------------------------------------------
\subsection*{Inner Products}
Inner products are symmetric and bilinear form in $\mathbb{R}^n$:
%
\begin{itemize}
\item bi-linear (depends on two arguments) $< \vec{x}, \vec{y} >$
\item symmetric $< \vec{x}, \vec{y} > = < \vec{y}, \vec{x} >$
\item linear $< \vec{x} + \alpha \vec{z}, \vec{y} > = < \vec{x}, \vec{y} > + \alpha < \vec{y}, \vec{z} >$
\item positive $< \vec{x}, \vec{x} >$ $>$ 0 for $\vec{x} \neq 0$
\end{itemize}

The dot product is also known as the Euclidean inner product. One can define many inner products, but \textit{in this class assume we're using the Euclidean inner product} (dot product) unless otherwise specified. 

\subsection*{Convergence}
We can use the concepts of norms and inner products to develop some relationships and talk about convergence. 

The \textit{Cauchy-Schwartz} inequality states
%
\begin{equation}
< \vec{x}, \vec{y} > \leq ||\vec{x}||_2 ||\vec{y}||_2 \nonumber
\end{equation}
%
Two norms, denoted $|| \cdot ||_a$ and $|| \cdot ||_b$ here, are defined as being equivalent if there exists $C_1$ and $C_2$ such that $\forall \vec{x} \in \mathbb{R}^n$
%
\begin{equation}
C_1 ||\vec{x}||_a \leq ||\vec{x}||_b \leq C_2 ||\vec{x}||_a \qquad \text{where } C_1, C_2 = f(n) \nonumber
\end{equation}
%
This leads to the theorem that in $\mathbb{R}^n$ all norms are equivalent (offered without proof). E.g.:
\begin{align}
& ||\vec{x}||_{\infty} \leq ||\vec{x}||_2 \leq \sqrt[]{n} ||\vec{x}||_{\infty} \nonumber \\
%
& ||\vec{x}||_{\infty} \leq ||\vec{x}||_1 \leq n ||\vec{x}||_{\infty} \nonumber \\
%
& ||\vec{x}||_2 \leq ||\vec{x}||_1 \leq \sqrt[]{n} ||\vec{x}||_2 \nonumber
\end{align}

The inequalities above may not be `sharp'. A sharp inequality is when there could be no better inequality when making the comparison. 

E.g., when comparing two real numbers/expressions, an inequality is sharp because we could not increase the left or decrease the right by a positive factor and still have it be true ($2 \leq 2$). 

%\textbf{Example}: let's show the last item. We will use the Signum function, which extracts the sign of a real number
%\begin{equation}
%\text{sign}(x) \equiv \begin{cases}
%  -1 & \text{ if } x < 0 \nonumber \\
%  0  & \text{ if } x = 0 \nonumber \\
%  1  & \text{ if } x > 0 \nonumber
%\end{cases}
%\end{equation}
%%
%Let's define $v_i$ as sign$(x_i)$. Then
%%
%\begin{align}
%&< \vec{x}, \vec{v} > = \sum_i x_i v_i = \sum_i x_i \text{sign}(x_i) = \sum_i |x_i| = ||\vec{x}||_1 \nonumber \\
%%
%&\text{but by the Cauchy-Schwartz inequality,}\nonumber \\
%%
%&< \vec{x}, \vec{v} > \leq ||\vec{x}||_2 ||\vec{v}||_2 \nonumber \\
%%
%&\text{and we can give an upper bound of } ||\vec{v}||_2 \leq \sqrt{n} \text{(the right side of the inequality);} \nonumber \\ 
%%
%&\text{we can also say } ||\vec{x}||_2 \leq ||\vec{v}||_2 ||\vec{x}||_2 \leq \sqrt{n} ||\vec{x}||_2 \nonumber \\
%%
%&\text{thus } ||\vec{x}||_2 \leq ||\vec{x}||_1 \leq \sqrt{n} ||\vec{x}||_2 \nonumber
%\end{align}
%
%NEED $||\vec{x}||_2 \leq ||\vec{x}||_1$ !!!!

Norm equivalence is very important because if we can prove some property (usually convergence or an error bound) in \textbf{some} norm, we have effectively proven it in \textbf{all} norms. 

\subsubsection*{Error}
For $\vec{x}$ (the real solution) and $\hat{x}$ (representing our numerical solution) $\in \mathbb{R}^n$ and some norm $p$ we define
%
\begin{itemize}
\item Absolute error: $||\hat{x} - \vec{x}||_p$
\item Relative error: $||\hat{x} - \vec{x}||_p / ||\vec{x}||_p$, where $\vec{x} \neq 0$
\end{itemize}

\subsubsection*{Convergence}
Given a sequence $\lbrace \hat{x}^{(k)} \rbrace_{k=1,2,\dots,\infty}$ and some norm $p$, we say that $\lbrace \hat{x}^{(k)} \rbrace$ converges to $\vec{x}$ if
%
\begin{equation}
\displaystyle\text{lim}_{k \rightarrow \infty} ||\hat{x}^{(k)} - \vec{x}||_p = 0 \nonumber
\end{equation}


%--------------------------------------------------------------------------
\subsection*{Matrix Norms}
Given $\ve{A}, \ve{B} \in \mathbb{R}^{m \times n}$, a matrix norm, denoted by $|| \cdot ||$, has the following properties:
%
\begin{enumerate}
\item $||\ve{A}|| > 0$ and $||\ve{A}|| = 0$ iff $\ve{A} = 0$ (positive definite)
\item $||\ve{A} + \ve{B}|| \leq ||\ve{A}|| + ||\ve{B}||$ (triangle inequality)
\item $||\alpha \ve{A}|| = |\alpha| ||\ve{A}||$ (homogeneous)
\end{enumerate}

E.g., the Frobenius norm 
%
\begin{equation}
||\ve{A}||_F = \sqrt{ \sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2 } \nonumber
\end{equation}

\textbf{Definitions}:
\begin{itemize}
\item submultiplicative if $||\ve{A} \ve{B}|| \leq ||\ve{A}|| ||\ve{B}||$

\item Not all norms are submultiplicative, we will only deal with those that are. 

\item subordinate matrix norm for $\ve{A} \in \mathbb{R}^{m \times n}$ and $\vec{x} \in \mathbb{R}^n$:

$||\ve{A}|| \equiv \displaystyle \sup_{\vec{x} \neq \vec{0}} ||\ve{A}\vec{x}|| / ||\vec{x}||$

\item \textbf{supremum}, or least upper bound, of a set $S$ of real numbers is denoted by sup$S$ and is defined to be the smallest real number that is greater than or equal to every number in $S$.

Consequently, the supremum is also referred to as the least upper bound (or LUB). If the supremum exists, it is unique, meaning that there will be only one supremum. 
\end{itemize}


\textbf{examples:}
%
\begin{align}
||\ve{A}||_{1} &= \displaystyle \sup_{\vec{x} \neq \vec{0}} \frac{||\ve{A}\vec{x}||_{1}}{||\vec{x}||_{1}} =
\displaystyle \max_{1 \leq j \leq n} \sum_{i=1}^m |a_{ij}| \qquad \text{max absolute col sum} \nonumber \\
%
||\ve{A}||_{\infty} &= \displaystyle \sup_{\vec{x} \neq \vec{0}} \frac{||\ve{A}\vec{x}||_{\infty}}{||\vec{x}||_{\infty}} = 
\displaystyle \max_{1 \leq i \leq m} \sum_{j=1}^n |a_{ij}| \qquad \text{max absolute row sum}\nonumber \\
%
||\ve{A}||_{2} &= \displaystyle \sup_{\vec{x} \neq \vec{0}} \frac{||\ve{A}\vec{x}||_{2}}{||\vec{x}||_{2}} = \text{ the sqrt of the max  eigenvalue of }\ve{A}^H\ve{A} \nonumber \\
&\text{ (largest singular value for any matrix) or the spectral radius of a square matrix} \nonumber 
\end{align}

For vectors, the infinity norm is the largest value. For matrices, it's the maximum absolute row sum - which is really the row that will most strongly change a vector, so that's analogous. For the 1 matrix norm, the maximum absolute column sum corresponds to the vector component that will, overall, be changed the most. We'll talk about the last after we talk about spectral radius.

The equivalence of norms we talked about with vectors also holds here. Some of the relationships are
%
\begin{align}
||\ve{A}||_{2} &\leq ||\ve{A}||_{F} \leq \sqrt{n}||\ve{A}||_{2} \nonumber \\
%
\frac{1}{\sqrt{n}}||\ve{A}||_{\infty} &\leq ||\ve{A}||_{2} \leq \sqrt{m}||\ve{A}||_{\infty} \nonumber \\
%
\frac{1}{\sqrt{m}}||\ve{A}||_{1} &\leq ||\ve{A}||_{2} \leq \sqrt{n}||\ve{A}||_{2} \nonumber
\end{align}

Again, as with vector norms, showing a property in some matrix norms implies that property in other matrix norms (which may be harder to compute). The inequalities above may not be `sharp'.

 %--------------------------------------------------------------------------
\subsection*{All matrix norms are bounded}

For $\lambda \in \sigma(\ve{A})$ 
\[|\lambda | \leq ||\ve{A}||\]
\[\rho(\ve{A}) \leq ||\ve{A}||\]

\underline{proof:} 
\begin{enumerate}
\item The eigenvalues of $\ve{A}$ satisfy $\ve{A}\vec{x} = \lambda x$, $\vec{x} \neq \vec{0}$. 

\item Let the eigenvectors of $\ve{A}$ be $\vec{v} = [\vec{v_1}, \vec{v_2}, \dots, \vec{v_n}]$

\item then 
\begin{align}
|\lambda | ||\vec{v}|| &= ||\lambda \vec{v}|| 
\qquad \text{property of norms} \nonumber \\
%
                     &= ||\ve{A}\vec{v}|| 
                     \qquad \text{substitution} \nonumber \\
%
                     &\leq ||\ve{A}|| ||\vec{v}||
                     \qquad \text{triangle inequality} \nonumber \\
%
\therefore |\lambda | &\leq ||\ve{A}|| 
\qquad \text{cancellation}\nonumber \\
%
|\lambda| &\leq \rho(\ve{A}) \leq ||\ve{A}|| 
\qquad \text{definition of }\rho\text{ as max }|\lambda | \nonumber
\end{align}
\end{enumerate}

Note, we can use any submultiplicative matrix norm to bound from above the spectral radius (this will be important).


\end{document}